
% ===========================================================================
% Title:
% ---------------------------------------------------------------------------
% to create Type I fonts type "dvips -P cmz -t letter <filename>"
% ===========================================================================
\documentclass[11pt]{article}       %--- LATEX 2e base
\usepackage{latexsym}               %--- LATEX 2e base
%---------------- Wide format -----------------------------------------------
\textwidth=6in \textheight=9in \oddsidemargin=0.25in
\evensidemargin=0.25in \topmargin=-0.5in
%--------------- Def., Theorem, Proof, etc. ---------------------------------
\newtheorem{definition}{Definition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{property}{Property}
\newtheorem{observation}{Observation}
\newtheorem{fact}{Fact}
\newenvironment{proof}           {\noindent{\bf Proof.} }%
                                 {\null\hfill$\Box$\par\medskip}
%--------------- Algorithm --------------------------------------------------
\newtheorem{algX}{Algorithm}
\newenvironment{algorithm}       {\begin{algX}\begin{em}}%
                                 {\par\noindent --- End of Algorithm ---
                                 \end{em}\end{algX}}
\newcommand{\step}[2]            {\begin{list}{}
                                  {  \setlength{\topsep}{0cm}
                                     \setlength{\partopsep}{0cm}
                                     \setlength{\leftmargin}{0.8cm}
                                     \setlength{\labelwidth}{0.7cm}
                                     \setlength{\labelsep}{0.1cm}    }
                                  \item[#1]#2    \end{list}}
                                 % usage: \begin{algorithm} \label{xyz}
                                 %        ... \step{(1)}{...} ...
                                 %        \end{algorithm}
%--------------- Figures ----------------------------------------------------
\usepackage{graphicx}

\newcommand{\includeFig}[3]      {\begin{figure}[htb] \begin{center}
                                 \includegraphics
                                 [width=4in,keepaspectratio] %comment this line to disable scaling
                                 {#2}\caption{\label{#1}#3} \end{center} \end{figure}}
                                 % usage: \includeFig{label}{file}{caption}


% ===========================================================================
\begin{document}
% ===========================================================================

% ############################################################################
% Title
% ############################################################################

\title{--- Parallel String Matching ---}


% ############################################################################
% Author(s) (no blank lines !)
\author{
% ############################################################################
Dengyu Liang\\
School of Computer Science\\
Carleton University\\
Ottawa, Canada K1S 5B6\\
{\em dengyuliang@cmail.carleton.ca}
% ############################################################################
} % end-authors
% ############################################################################

\maketitle

% ############################################################################
% Abstract
% ############################################################################
\begin{abstract}
This project explores and implements CPU-parallel over Cilk and GPU-parallel over Cuda of 6 different string-matching algorithms and evaluates the performance of each algorithm. Each algorithm was evaluated on three different sets of data: natural language text, genomic data, and program-generated data. The results showed that most algorithms can benefit from CPU parallelism, achieving 3-10x speedup in 16 threads. GPU performs poorly on most files less than 100MB and only has a huge speedup on special data, such as the 1000 repeat letters on the brute force algorithms.  Based on the CPU and GPU in the experiment, GPU has a 2x speedup on the KMP algorithms, and 2/3 Cuda cores speedup in the 1000 repeat letters on the brute force algorithms. 

\end{abstract}


% ############################################################################
\section{Introduction} \label{intro}
% ############################################################################

String Matching is one of the most common problems in computer science. Although sequence algorithms work very well on this problem, the algorithm speed is affected by the linear increase in the size of the matching string, especially when solving Bioinformatics DNA Sequencing, search engines, or content search in large databases, searching in a large amount of data inevitably takes a lot of time. Modern CPUs tend to have a large number of cores, and it is increasingly common to utilize GPUs for data processing. Parallelization has become an important part of algorithm design. Because of the data structure of String Matching, it can take benefits from Parallel Programming to solve large-scale Matching problems efficiently on large-scale systems.\\
\\This project is to explore the parallel implementation of traditional sequence string matching algorithms and implement its parallel version over Cilk and Cuda, then compare the benefit of different string matching algorithms in different parallel architectures and the applications and limitations on a distributed architecture. In this project, I implement six different string-matching algorithms in CPU parallel and three in GPU. For algorithms that are difficult to algorithmically parallelize, I partition the index to perform parallel operations. Some of the algorithms take also use bit parallel or SSE instruction to speed up. I analyze The speedup obtained by different algorithms, and the speedup obtained in the worst case.\\ 
\\A more effective algorithm can be selected through pattern and text length analysis and preprocessing. Our results also discuss this point. The data type also has an impact on efficiency. A genomic sequence with four letters has more repetitions than natural language, and this repetition will also greatly affect efficiency. We'll also compare different datasets and explore the differences to determine the best algorithm.\\




% ############################################################################
\section{Literature Review} \label{litrev}
% ############################################################################

String Matching is an important problem in computer science, and many people continue to study it. The most recent comparison was Philip Pfaffe, who discussed 7 parallelized string matching algorithms based on SIMD\cite{Matching}. There are different computer architectures according to Flynn classification, and for different architectures, there have different methods to minimize latency and improve performance. Moreover, the CPU and GPU architectures also have different memory hierarchies, which need to be taken into account when designing string-matching algorithms. This project compares the advantages and disadvantages of different architectures in solving string-matching problems and measures the performance of CPU parallelism and GPU parallelism. \\


\includeFig{fig1}{Figures/example}{String Matching wants to find the pattern from Text}
% usage: \includeFig{label}{file}{caption}

\subsection{Sequential String Matching}\label{Simple}
String Matching is one of the common fundamental problems, and many algorithms have been proposed to solve it. Faster solutions were proposed as early as 1977, the KMP algorithm and the Boyer–Moore algorithm\cite{Matching}. Since 1970, more than 80 String Matching algorithms have been proposed. These algorithms use preprocessing techniques to speed up the search process.  In addition, algorithms such as the edit distance algorithm and suffix tree can be used to solve more complex string-matching problems. Based on previous research those algorithm deformations, combinations, and expansion, String Matching has been continuously optimized. Now, using parallelized algorithms to improve performance is a more mainstream method\cite{Matching}, by running the traditional algorithm in parallel, the original algorithm can be accelerated. The specific algorithm is listed in section 4 Implementation.. By exploring the details of these algorithms, the efficiency of parallelized algorithms can be further improved. \\

\subsection{Parallel String Matching}\label{Multiple}
Parallel computing has great potential and extremely high scalability. Making good use of parallel computing can greatly speed up the calculation speed. Although not all algorithms can benefit from parallel computing. Specific to the String Matching algorithm, It can be parallelized from data input, It is also possible to parallelize certain operations. In conclusion, parallel computing has great potential and can be used to speed up the calculation speed of string-matching algorithms. However, depending on the algorithm, the best core utilization can vary and it is important to analyze the speedup achieved in different numbers of threads.\\

\subsubsection{Parallel String Matching base on MISD}
Although there exist MISD architectures dedicated to pattern matching, such as Halaas's recursive architecture \cite{MISD}, which can be used to solve string-matching problems, its performance is not competitive compared to other solutions. It can match up to 127 patterns at a clock frequency of 100 MHz, but this is not much better than a 1GHZ single-core CPU. Moreover, MISD processors lack practical applications, and research in this area has been shelved.


\subsubsection{Parallel String Matching base on SIMD}
SIMD uses one instruction stream to process multiple data streams. A typical example is the units on the GPU. Most Parallel String Matching benefits from SIMD architecture. Such as Chinta Someswararao's Butterfly Model \cite{Butterfly}, and some string matching algorithms are developed based on the SSE instruction set \cite{Matching}, for example, the SSEF algorithm. which is a SIMD instruction set developed by Intel. \\

\subsubsection{Parallel String Matching base on GPU(CUDA)}
Compared with the CPU, modern GPU has huge advantages in parallel computing. A single GPU integrates thousands of computing units, so thousands of parallel computing can be realized on a single GPU. There are also some ways to use CUDA programming provided by Nvidia GPU for String Matching. Giorgos Vasiliadis implements string search and regular expression matching operations for real-time inspection of network packets through the pattern matching library \cite{Bit-Parallel}, And in 2011 it reached a data volume close to 30 Gbit/s. Efficient GPU algorithms can be 30 times faster than a single CPU core, and Approximate String Matching with k Mismatches reports an 80 times speedup \cite{pattern-matching}. This makes GPU computing have certain advantages in cost and speed, but the research in this area is not as much as the traditional string algorithm. \\


\subsubsection{Parallel String Matching base on MIMD}
MIMD machines can execute multiple instruction streams at the same time. Many modern CPUs belong to this type. In order to fully mobilize multiple instruction streams, targeted parallel algorithms are inevitable. There are not many studies in this area. Hitoshi developed an algorithm called PZLAST used for MIMD processor PEZY-SC2 and compared the performance of BLAST+, CLAST, and PZLAST algorithms\cite{MIMD}, which are specially optimized for the biological field. However, they didn't explore the percentage of parallel utilization of the algorithm.\\


\subsubsection{Distributed Memory Parallel String Matching}
There are few articles discussing the String Matching algorithm on Shared and Distributed-Memory Parallel Architectures, except Antonino Tumeo's Aho-Corasick algorithm in 2012 compared and analyzed the distributed memory architecture and shared memory architecture\cite{Distributed-Memory}. Results at the time showed that shared-memory architectures based on multiprocessors were the theoretical best performance, but there is an upper limit to the amount of parallelism, at 80 threads he starts to get degraded speedup, and beyond 96 threads the speedup becomes marginal. considering cost constraints, GPUs that did not reach the PCI-Express bandwidth limit had the best price/performance ratio. The performance of GPU has developed several times in the past ten years, and theoretically, its performance is far faster than that of CPU now. Although distributed can provide sufficient space and computing resources, limited by the cost of communication, the performance of distributed computing is not satisfactory. Especially for a single String Matching problem. Nonetheless, this can lead to considerable performance gains for multiple patterns search, as shown by Panagiotis' work\cite{MPI}, In a 16-processor cluster, the matching time can be reduced by half of the original.\\



% ############################################################################
\section{Problem Statement} \label{problemStatement}
% ############################################################################


A definition of the problem of string matching is to find a pattern P in a text T. we define a pattern that has length p and text that has length t. The pattern and text are based on the same alphabet. The results are the positions of every occurrence of P in T. The number of threads used is c. The input is dynamic, runtime should include the time cost of preprocessing of pattern or text . Only exact matches have to found, I'm not considered to find the approximate matches or regular expression patterns. 



% ############################################################################
\section{Implementation} \label{proposedSolution}
% ############################################################################

The algorithms we explore range from the most basic brute force algorithms, and partial optimization algorithms to modern advanced algorithms using SSE and bit parallelism. In the parallel implementation of the algorithm, except for the Brute Force algorithm, which is a direct parallel loop, the CPU of other algorithms uses the method of index segmentation in parallel, so that each thread is allocated to a string of( t/c + p), thereby realizing parallel operations. The GPU only implements the parallel implementation of Brute Force and Boyer-Moore. And try to parallelize the KMP algorithm by allocating a fixed length of string to slice. Some of the algorithms use the SSE instruction set to improve bit parallelism. The following subsections provide a brief overview of the implemented algorithms.


\subsection{Brute Force}\label{bf}
The basic Brute Force algorithm compares the pattern to the text, one character at a time until unmatching characters are found. Brute Force means the worst algorithm in most problems. But that's not exactly the case in String Matching Problem. In the average case, Brute Force takes O (P + T) steps. The result reflected in the experiment is that in some natural language searches, the Brute Force algorithm is faster than some algorithms. But there's no way to avoid bad worst-case outcomes. In the worst case, the Brute Force algorithm requires (p*t) steps, which is unacceptable in large searches. Many algorithms benefit from the pattern to avoid the worst case but brute force algorithms do not.

\subsection{KMP algorithm}\label{kmp}
KMP algorithm, maybe the first optimization algorithm\cite{KMP} for string match algorithm, KMP calculates the number of times it can skip each time it reads a string, It uses a preprocessing phase on the pattern to build a partial match table. Although this avoids the worst result, In theory, KMP has a time cost of O(t), but due to the calculations at the same time, KMP can’t speed up too much. Excellent performance in some cases, but doesn’t take high scores in most cases. relatively, due to more calculations, it gets more benefit from parallelism.

\subsection{Boyer-Moore}\label{bm}
Boyer-Moore algorithm calculates the bad character rule and the good suffix rule\cite{BM}, matches from the last index to the front, and skips unmatch string by rule. Therefore, it can skip some unmatched results and thus exceed linear time in algorithm analysis for most natural languages and large pattern capacity matching problems with better speedup than KMP. it has (p) Preprocessing time and  (t/p) at best, O(p*t) at the worst Matching time, and requires (p) space to store the good and bad rule.

\subsection{Rabin-Karp}\label{rk}
The Rabin-Karp algorithm gets inspiration from the hash function. When scanning strings for comparison, it uses the hash value generated by Rolling hash comparison instead of directly comparing strings\cite{KR}. This avoids directly scanning the entire pattern. Only strings with the same hash can be compared. A full scan will be performed. Because the hash is fixed, the speed of the algorithm depends on the hash function and the string alphabet. The worst result is still mn, but as a heuristic algorithm, it is faster than the expected value of the brute force algorithm.

\subsection{Shift-Or}\label{so}
The Shift Or algorithm uses bitwise techniques\cite{Matching}, it keeps an array of bits, R, showing if prefixes of the pattern don't match at the current place. Before searching, mismatch arrays are computed for each character in the alphabet and saved in an array, S. For the next position, with the character c, R = shift(R) or S[c]. If the last bit of R is 0, the pattern matches. The runtime is deterministic and in O(n*m). Nevertheless, it is a fast operation due to bit parallelism.

\subsection{SSEF}\label{SSEF}
The SSEF algorithm precomputes 65536 filter lists based on the kth bit of each character on the patterns\cite{Matching}. These filters are then applied efficiently, utilizing SSE instructions, on shifting alignments of pattern and text. SSEF is restricted to patterns with a minimum length of m greater than 32. The worst-case runtime is in O(n*m). If we consider the probability to filter possible matches, SSEF achieves an average runtime in O(n*m/65536). The SSEF algorithm doesn't exactly cover all implementations, but it's a good comparison since Philip Pfaffe claims it has the best performance on their evaluation\cite{SSEF}.

\subsection{Exact-Packed-String-Matching}\label{EPSM}
Exact-Packed-String-Matching (EPSM) was presented by Faro and Külekc in 2013\cite{EPSM}. EPSM packing several characters into a bit-word based on the Intel streaming SIMD extensions and compared with a packed pattern bit-word. it uses compare idea like shift or algorithm, shift and bitwise-and operations are used to efficiently compare text chunks with the pattern. The asymptotic runtime of the algorithm is O(p*t), but in practice very fast.


% usage: \includeFig{label}{file}{caption}

\includeFig{fig2}{Figures/fig2}{Runtime of seven algorithms for 1,2,4, 8, 16, 32, and 64 patterns. natural language text (bible).}

\includeFig{fig3}{Figures/fig3}{Runtime of seven algorithms for 1,2,4, 8, 16, 32, and 64 patterns. genome data set.}
\includeFig{fig4}{Figures/fig4}{Runtime of five algorithms for 1,2,4, 8, 16, and 1000 patterns.complex generated data set.}
% ############################################################################
\section{Experimental Evaluation} \label{expEval}
% ############################################################################

In this section, I present the performance evaluation of parallel string matching algorithms. The sequential implementation of some codes comes from String Matching Algorithms Research Tool, then modified to accommodate multi-threading requirements.\\
Here are the datasets I used for comparison:\\


A sample of natural language (the text of the Bible, 3.1MB) 

A set of genome sequences (consisting of four letters alphabet {A,T,C,G}, 98MB)

Extreme data generated by some programs (consisting only two letters alphabet {a,b} and only 1 b,1GB)\\

In order to evaluate the benefits of parallelization, the input file is directly read into memory completely to reduce I/O latencies, and then my algorithm is run on a number of 1, 2, 4, 8, 16, 32 threads, and their respective running times are recorded. And test patterns with lengths of 2, 4, 8, 16, 32, 64 and 1000.\\
All CPU parallel experiments were performed on AMD ryzen9 3950x and compiled by opencilk, link sse4.2. The CPU has 16 CPU cores (32 hardware threads) clocked at 4.1GHz(overclock). The GPU is based on nVidia RTX 2070 8GB running on the latest CUDA 22.04.\\
In the following subsection, I discuss an excerpt of my result data.
\includeFig{fig5}{Figures/tbible}{Runtime of four algorithms for 1,2,4, 8, 16, and 32 threads. natural language text, pattern length of 32.}
\includeFig{fig6}{Figures/ngeno}{Runtime for 1 and 32 threads and gpu. Genome data set, pattern length of 32.}

\includeFig{fig7}{Figures/fig5}{Runtime of seven algorithms for 1, 32 threads and gpu. complex generated data set, pattern length of 1000.}
\subsection{Pattern}\label{pf}

This is about the impact of patterns on parallel performance. When I want to analyze the benefits of threads, I want to find a relatively stable pattern as an input. Figure~\ref{fig2}~\ref{fig3}~\ref{fig4} is our test result, we can see that in our test, it is indeed affected Very short pattern (pattern less than 8) does have a certain impact on the algorithm, but when a pattern greater than 8, only the Boyer-Moore algorithm benefits from it, and the impact on other algorithms is marginal. \\
However, in the worst case, brute force and EPSM increase the running time significantly with the increase of patterns, while the KMP algorithm remains stable. This is due to the fact that EPSM partitions the text T into blocks to use bit parallelism compared to packed mode bit words. Due to the characteristics of memory comparison, the length of each comparison using bit parallelism is fixed, and the advantages cannot be utilized for large patterns., a disadvantage of using bit parallelism. However, KMP does not increase the running time because it can jump enough distance each time. Of course, in this case, the running time of the brute force algorithm reached 15 minutes, which is why we cannot directly use the brute force algorithm, the running time, in this case, is unacceptable.

\subsection{Thread}\label{tf}


We focus on the impact of threads on algorithms or different speedup ratios.  Figure~\ref{fig4} shows the variation of the running time of the matching algorithm in the Bible text under different numbers of threads, most algorithms are speedup under multi-core, except EPSM, but the speed of EPSM on the parallel has over the sequence. The running speed on CUDA in some algorithms is slightly better than sequence, but it is difficult to exceed the parallel speedup of 16 threads.  Figure~\ref{fig5} is the data under the genome file,  due to the nature of the genes, A large number of repeats in the small dataset {ATCG}, the running time of the brute force algorithm is increased, while the other algorithms remain stable.  Figure~\ref{fig6} shows the poor data being generated. In this case, the brute force algorithm takes 15 minutes, most algorithms take seconds, and the fastest algorithm is Shift Or and KR, less than a second. In this case, we can see a significant speedup, with all algorithms achieving a speedup of more than 5X speed up in a 32-thread parallel. The time spent by Cuda is much faster than that of CPU parallel in this case. In particular, the brute force algorithm is actually 0.3 seconds, it is over 1500x speedup. This speedup ratio has way higher than all sequential algorithms, which means that under normal text, the GPU can hardly exert its performance, and everything is done.


\includeFig{fig8}{Figures/sbible}{Speedup of seven algorithms for 1,2,4, 8, 16, and 32 threads. natural language text, pattern length of 1.}
\includeFig{fig9}{Figures/genospeedup}{Speedup of seven algorithms for 1,2,4, 8, 16, and 32 threads. Genome data set, complex generated data sett, pattern length of 32.}
\includeFig{fig10}{Figures/sworst}{Speedup of seven algorithms for 1,2,4, 8, 16, and 32 threads. Genome data set, pattern length of 32.}

\subsection{Speed up}\label{su}

Analyzing which algorithm has a good speedup is the main purpose of this project. Figure~\ref{fig8}~\ref{fig9}~\ref{fig10} show the speedup ratios achieved in different types of data, respectively. Figure 8 shows the speedup ratio in general text, the speed-up ratio is not high. On average, the speedup ratio can only reach less than half of the number of cores. When there are 32 threads, the performance of the RabinKrap algorithm even drops slightly, while the KMP algorithm achieves a good speedup ratio. And the brute force algorithm speeds up poorly, only achieving 2.3X speedup in 32 threads\\ 
Figure 9 shows the speedup ratio on the gene. Except for EPSM, the algorithms have achieved good speedup ratios, but EPSM has brought delays. KMP algorithm still achieved good speedup ratios, even 14X speedup ratio in 32 threads. Brute Force, SSEF, and Boyer-Moore have a speedup of 6-8X, while Shift-Or and Rabin-Karp are 4 and 5 times respectively.\\
In the worst text shows in Figure 10, the brute force algorithm has achieved a good speedup ratio, which is not without cost, and the time spent dropping from 10 minutes to 40 seconds is still unacceptable. In contrast, the speedup achieved by KMP and RabinKrap is not high, but it is far more than other algorithms, KMP even came to third place in speed in this case.


% usage: \includeFig{label}{file}{caption}






% ############################################################################
\section{Conclusions} \label{concl}
% ############################################################################

I sliced the index according to the number of threads to do parallel versions of six different string-matching algorithms, and compared to the version on the GPU, I tested on different types and sizes of datasets.  Most algorithms can benefit from CPU parallelism except for EPSM. achieving 3-10x speedup in 16 threads. Brute force algorithms benefit most from parallelism, and parallel brute force algorithms perform best on most natural texts, Followed by EPSM. For large patterns, the winner is Rabin-Karp. While sequential EPSM has the best performance in genetics and generated text, it cannot benefit from parallelism except for the worst case. In the worst case, the GPU best achieves a speedup that matches the number of Cuda cores in the brute-force algorithm, on the average normal text has a 4x speedup. In the general case, the algorithm only has a 2x speedup on the GPU. I observed the effect of the pattern size on the string-matching algorithm. For short patterns, EPSM has far better performance than parallel performance, In my case, only the worst case is seen to speed up the algorithm by parallelism. while for 1000 patterns EPSM cannot achieve to win. This is because the EPSM using bit parallelism only needs one instruction to compare within the range allowed by the memory, and the excess part still needs to be searched by the loop. In this case Rabin-Karp and GPU-parallelized brute force algorithms are the clear winners.\\
\\Therefore, through the analysis of the pattern, it's able to select a better method for the String Matching problem. In the future, I plan to analyze the pattern and provide the optimal solution by choosing a better method for string comparison.





% ############################################################################
% Bibliography
% ############################################################################
\begin{thebibliography}{99}  
\bibitem{Matching}Pfaffe, P., Tillmann, M., Lutteropp, S., Scheirle, B., Zerr, K. (2017). Parallel String Matching. In: , et al. Euro-Par 2016: Parallel Processing Workshops. Euro-Par 2016. Lecture Notes in Computer Science(), vol 10104. Springer, Cham. {https://doi.org/10.1007/978-3-319-58943-5 15}
\bibitem{Bit-Parallel}C. -L. Hung, T. -H. Hsu, H. -H. Wang and C. -Y. Lin, "A GPU-based Bit-Parallel Multiple Pattern Matching Algorithm," 2018 IEEE 20th International Conference on High Performance Computing and Communications; IEEE 16th International Conference on Smart City; IEEE 4th International Conference on Data Science and Systems (HPCC/SmartCity/DSS), 2018, pp. 1219-1222, doi: 10.1109/HPCC/SmartCity/DSS.2018.00205.
\bibitem{pattern-matching}G. Vasiliadis, M. Polychronakis and S. Ioannidis, "Parallelization and characterization of pattern matching using GPUs," 2011 IEEE International Symposium on Workload Characterization (IISWC), 2011, pp. 216-225, doi: 10.1109/IISWC.2011.6114181.
\bibitem{NFA}Yuan Zu, Ming Yang, Zhonghu Xu, Lin Wang, Xin Tian, Kunyang Peng, and Qunfeng Dong. 2012. GPU-based NFA implementation for memory efficient high speed regular expression matching. SIGPLAN Not. 47, 8 (August 2012), 129–140. https://doi.org/10.1145/2370036.2145833
\bibitem{Approximate}Y. Liu, L. Guo, J. Li, M. Ren and K. Li, "Parallel Algorithms for Approximate String Matching with k Mismatches on CUDA," 2012 IEEE 26th International Parallel and Distributed Processing Symposium Workshops PhD Forum, 2012, pp. 2414-2422, doi: 10.1109/IPDPSW.2012.298.
\bibitem{MISD}A. Halaas, B. Svingen, M. Nedland, P. Saetrom, O. Snove and O. R. Birkeland, "A recursive MISD architecture for pattern matching," in IEEE Transactions on Very Large Scale Integration (VLSI) Systems, vol. 12, no. 7, pp. 727-734, July 2004, doi: 10.1109/TVLSI.2004.830918.
\bibitem{kmp}Knuth, D. E., Morris, Jr, J. H., Pratt, V. R. (1977). Fast pattern matching in strings. SIAM journal on computing, 6(2), 323-350.
\bibitem{MPI}P. D. Michailidis and K. G. Margaritis, "Performance Evaluation of Multiple Approximate String Matching Algorithms Implemented with MPI Paradigm in an Experimental Cluster Environment," 2008 Panhellenic Conference on Informatics, 2008, pp. 168-172, doi: 10.1109/PCI.2008.13.
\bibitem{Butterfly}Someswararao, Chinta. (2012). Parallel Algorithms for String Matching Problem based on Butterfly Model. International Journal of Computer Science and Technology. 
\bibitem{Distributed-Memory}A. Tumeo, O. Villa and D. G. Chavarria-Miranda, "Aho-Corasick String Matching on Shared and Distributed-Memory Parallel Architectures," in IEEE Transactions on Parallel and Distributed Systems, vol. 23, no. 3, pp. 436-443, March 2012, doi: 10.1109/TPDS.2011.181.
\bibitem{MIMD}H. Ishikawa et al., "PZLAST: an ultra-fast sequence similarity search tool implemented on a MIMD processor," 2021 Ninth International Symposium on Computing and Networking (CANDAR), 2021, pp. 102-107, doi: 10.1109/CANDAR53791.2021.00021.
\bibitem{KMP}Knuth, Donald E., James H. Morris, Jr, and Vaughan R. Pratt. "Fast pattern matching in strings." SIAM journal on computing 6.2 (1977): 323-350.
\bibitem{KR}Karp, Richard M., and Michael O. Rabin. "Efficient randomized pattern-matching algorithms." IBM journal of research and development 31.2 (1987): 249-260.
\bibitem{BM}Boyer, Robert S., and J. Strother Moore. "A fast string searching algorithm." Communications of the ACM 20.10 (1977): 762-772.
\bibitem{SSEF}Külekci, M.. (2009). Filter Based Fast Matching of Long Patterns by Using SIMD Instructions.. Stringology. 118-128. 
\bibitem{EPSM}Faro, Simone, and M. Oğuzhan Külekci. "Fast packed string matching for short patterns." 2013 Proceedings of the Fifteenth Workshop on Algorithm Engineering and Experiments (ALENEX). Society for Industrial and Applied Mathematics, 2013.
\end{thebibliography}

% ============================================================================
\end{document}
% ============================================================================
